{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Renthop Notebook\n",
    "\n",
    "## Two Sigma Connect Renthop Competition\n",
    "\n",
    "---\n",
    "\n",
    "### Info\n",
    "\n",
    "Author: Theo Naunheim\n",
    "\n",
    "Date: 2017-04-22\n",
    "\n",
    "License:    MIT license\n",
    "\n",
    "Link: [https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)\n",
    "\n",
    "---\n",
    "\n",
    "### \"Strategy\"\n",
    "\n",
    "* Create feature probabilities based on 'features' text and 'description' text.\n",
    "\n",
    "* Create feature probabilities based on longitude and latitude DBSCAN clusters.\n",
    "\n",
    "* Create features based on perceived value.\n",
    "\n",
    "* Throw all the features at a boosted gradient classfier and let god sort them out.\n",
    "\n",
    "---\n",
    "\n",
    "### Inputs\n",
    "1. train.json (49,352 samples, 15 features)\n",
    "2. test.json (74,659 samples, 14 features)\n",
    "\n",
    "### Outputs\n",
    "1. renthop_predictions.csv (74,659 samples, 4 features)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Original matrix\n",
    "odf = pd.read_json('./train.json').set_index('listing_id')\n",
    "oy = odf['interest_level']\n",
    "oX = odf.drop('interest_level', axis=1)\n",
    "\n",
    "# Test matrix\n",
    "tX = pd.read_json('./test.json').set_index('listing_id')\n",
    "\n",
    "# Original derived matrix\n",
    "odX = pd.DataFrame(index=oX.index)\n",
    "\n",
    "# Test-derived matrix\n",
    "tdX = pd.DataFrame(index=tX.index)\n",
    "\n",
    "# All matrices.\n",
    "all_X = [oy, oX, odX, tX, tdX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:628: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(proba)\n"
     ]
    }
   ],
   "source": [
    "def create_features_probabilities(oy, oX, odX, tX, tdX):\n",
    "    '''Create probabilities for 'feature' column.'''\n",
    "\n",
    "    # Make text vectors\n",
    "    o_feat = oX['features'].map(lambda text_list: ' '.join(text_list))\n",
    "    t_feat = tX['features'].map(lambda text_list: ' '.join(text_list))\n",
    "\n",
    "    # Gridsearch Params (TODO, add more)\n",
    "    parameters = {'vec__ngram_range': [(1, 1)],\n",
    "                  'vec__min_df': [10, 15],\n",
    "                  'trans__use_idf': [True]}\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([('vec', CountVectorizer()),\n",
    "                         ('trans', TfidfTransformer()),\n",
    "                         ('clf', ExtraTreesClassifier())])\n",
    "\n",
    "    # Gridsearch\n",
    "    feat_clf = GridSearchCV(pipeline, parameters, n_jobs=3)\n",
    "\n",
    "    # Fit\n",
    "    feat_clf.fit(o_feat.values, oy.values)\n",
    "\n",
    "    # List classes and create friendly labels\n",
    "    feat_classes = feat_clf.best_estimator_.classes_\n",
    "    feat_headers = np.add(['text_feat_'], [feat_classes]).flatten()\n",
    "\n",
    "\n",
    "    # Get proba and add to original derived dataframe\n",
    "    tdf1 = pd.DataFrame.from_records(feat_clf.predict_log_proba(o_feat),\n",
    "                                     columns=feat_headers,\n",
    "                                     index=oX.index)\n",
    "    odX = pd.concat([odX, tdf1], axis=1)\n",
    "    \n",
    "    # Get proba and add to test derived dataframe.\n",
    "    tdf2 = pd.DataFrame.from_records(feat_clf.predict_log_proba(t_feat),\n",
    "                                     columns=feat_headers,\n",
    "                                     index=tX.index)\n",
    "    tdX = pd.concat([tdX, tdf2], axis=1)\n",
    "    \n",
    "    return [oy, oX, odX, tX, tdX]\n",
    "\n",
    "\n",
    "all_X = create_features_probabilities(*all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_feat_high</th>\n",
       "      <th>text_feat_low</th>\n",
       "      <th>text_feat_medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7142618</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210040</th>\n",
       "      <td>-3.954089</td>\n",
       "      <td>-0.096874</td>\n",
       "      <td>-2.615197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7103890</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.310155</td>\n",
       "      <td>-1.321756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7143442</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.182322</td>\n",
       "      <td>-1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860601</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text_feat_high  text_feat_low  text_feat_medium\n",
       "listing_id                                                 \n",
       "7142618               -inf       0.000000              -inf\n",
       "7210040          -3.954089      -0.096874         -2.615197\n",
       "7103890               -inf      -0.310155         -1.321756\n",
       "7143442               -inf      -0.182322         -1.791759\n",
       "6860601               -inf       0.000000              -inf"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_X[4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:628: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(proba)\n"
     ]
    }
   ],
   "source": [
    "def create_description_probabilities(oy, oX, odX, tX, tdX):\n",
    "    '''Create probabilities for 'feature' column.'''\n",
    "\n",
    "    # Make text vectors\n",
    "    o_desc = oX['description']\n",
    "    t_desc = tX['description']\n",
    "\n",
    "    # Gridsearch Params\n",
    "    parameters = {'vec__ngram_range': [(1, 3)],\n",
    "                  'vec__min_df': [10],\n",
    "                  'trans__use_idf': [True]}\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([('vec', CountVectorizer()),\n",
    "                         ('trans', TfidfTransformer()),\n",
    "                         ('clf', ExtraTreesClassifier())])\n",
    "\n",
    "    # Gridsearch\n",
    "    desc_clf = GridSearchCV(pipeline, parameters, n_jobs=3)\n",
    "\n",
    "    # Fit\n",
    "    desc_clf.fit(o_desc.values, oy.values)\n",
    "\n",
    "    # List classes and create friendly labels\n",
    "    desc_classes = desc_clf.best_estimator_.classes_\n",
    "    desc_headers = np.add(['text_desc_'], [desc_classes]).flatten()\n",
    "\n",
    "\n",
    "    # Get proba and add to original derived dataframe -> temp df 1\n",
    "    tdf1 = pd.DataFrame.from_records(desc_clf.predict_log_proba(o_desc),\n",
    "                                     columns=desc_headers,\n",
    "                                     index=oX.index)\n",
    "    odX = pd.concat([odX, tdf1], axis=1)\n",
    "    \n",
    "    # Get proba and add to test derived dataframe -> temp df 2\n",
    "    tdf2 = pd.DataFrame.from_records(desc_clf.predict_log_proba(t_desc),\n",
    "                                     columns=desc_headers,\n",
    "                                     index=tX.index)\n",
    "    tdX = pd.concat([tdX, tdf2], axis=1)\n",
    "    \n",
    "    return [oy, oX, odX, tX, tdX]\n",
    "\n",
    "\n",
    "all_X = create_description_probabilities(*all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_feat_high</th>\n",
       "      <th>text_feat_low</th>\n",
       "      <th>text_feat_medium</th>\n",
       "      <th>text_desc_high</th>\n",
       "      <th>text_desc_low</th>\n",
       "      <th>text_desc_medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7142618</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.356675</td>\n",
       "      <td>-1.203973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210040</th>\n",
       "      <td>-3.954089</td>\n",
       "      <td>-0.096874</td>\n",
       "      <td>-2.615197</td>\n",
       "      <td>-1.203973</td>\n",
       "      <td>-0.510826</td>\n",
       "      <td>-2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7103890</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.310155</td>\n",
       "      <td>-1.321756</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7143442</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.182322</td>\n",
       "      <td>-1.791759</td>\n",
       "      <td>-1.696449</td>\n",
       "      <td>-0.836248</td>\n",
       "      <td>-0.958850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860601</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text_feat_high  text_feat_low  text_feat_medium  text_desc_high  \\\n",
       "listing_id                                                                    \n",
       "7142618               -inf       0.000000              -inf            -inf   \n",
       "7210040          -3.954089      -0.096874         -2.615197       -1.203973   \n",
       "7103890               -inf      -0.310155         -1.321756            -inf   \n",
       "7143442               -inf      -0.182322         -1.791759       -1.696449   \n",
       "6860601               -inf       0.000000              -inf            -inf   \n",
       "\n",
       "            text_desc_low  text_desc_medium  \n",
       "listing_id                                   \n",
       "7142618         -0.356675         -1.203973  \n",
       "7210040         -0.510826         -2.302585  \n",
       "7103890         -2.302585         -0.105361  \n",
       "7143442         -0.836248         -0.958850  \n",
       "6860601          0.000000              -inf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_X[4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "def create_dbscan_df(oy, oX, odX, tX, tdX):\n",
    "    '''Create DBSCAN lookup database with probabilities.'''\n",
    "    # Get locations\n",
    "    locations = oX[['longitude', 'latitude']]\n",
    "\n",
    "    # First cluster using DBSCAN\n",
    "    cl_clf = DBSCAN(eps=.001, min_samples=10, n_jobs=3)\n",
    "    cluster_designations = cl_clf.fit_predict(locations.values)\n",
    "    \n",
    "    # Create an dataframe with temp info\n",
    "    tdf = pd.DataFrame({'interest_level': oy,\n",
    "                        'cluster': cluster_designations,\n",
    "                        'longitude': oX['longitude'],\n",
    "                        'latitude': oX['latitude']})\n",
    "    \n",
    "    # Calculate cluster centers\n",
    "    centers = tdf.groupby('cluster')[['longitude', 'latitude']].mean()\n",
    "\n",
    "    # Break out clusters by interest level\n",
    "    counts = tdf.groupby(['cluster', 'interest_level']).size()\n",
    "    count_df = counts.unstack().fillna(0)\n",
    "    proba_df = count_df.apply(lambda x: np.log(x/sum(x)), axis=1)\n",
    "\n",
    "    # Concat into df\n",
    "    dbscan_df = pd.concat([proba_df, centers], axis=1)\n",
    "    dbscan_df = dbscan_df.add_prefix('dbscan_avg_')\n",
    "    \n",
    "    # Create kd tree for faster indexing\n",
    "    kd_tree = KDTree(centers)\n",
    "    \n",
    "    return (dbscan_df, kd_tree)\n",
    "\n",
    "\n",
    "dbscan_df, kd_tree = create_dbscan_df(*all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_feat_high</th>\n",
       "      <th>text_feat_low</th>\n",
       "      <th>text_feat_medium</th>\n",
       "      <th>text_desc_high</th>\n",
       "      <th>text_desc_low</th>\n",
       "      <th>text_desc_medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7142618</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.356675</td>\n",
       "      <td>-1.203973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210040</th>\n",
       "      <td>-3.954089</td>\n",
       "      <td>-0.096874</td>\n",
       "      <td>-2.615197</td>\n",
       "      <td>-1.203973</td>\n",
       "      <td>-0.510826</td>\n",
       "      <td>-2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7103890</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.310155</td>\n",
       "      <td>-1.321756</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7143442</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.182322</td>\n",
       "      <td>-1.791759</td>\n",
       "      <td>-1.696449</td>\n",
       "      <td>-0.836248</td>\n",
       "      <td>-0.958850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860601</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text_feat_high  text_feat_low  text_feat_medium  text_desc_high  \\\n",
       "listing_id                                                                    \n",
       "7142618               -inf       0.000000              -inf            -inf   \n",
       "7210040          -3.954089      -0.096874         -2.615197       -1.203973   \n",
       "7103890               -inf      -0.310155         -1.321756            -inf   \n",
       "7143442               -inf      -0.182322         -1.791759       -1.696449   \n",
       "6860601               -inf       0.000000              -inf            -inf   \n",
       "\n",
       "            text_desc_low  text_desc_medium  \n",
       "listing_id                                   \n",
       "7142618         -0.356675         -1.203973  \n",
       "7210040         -0.510826         -2.302585  \n",
       "7103890         -2.302585         -0.105361  \n",
       "7143442         -0.836248         -0.958850  \n",
       "6860601          0.000000              -inf  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_X[4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dbscan_probabilities(oy, oX, odX, tX, tdX, dbscan_df, kd_tree):\n",
    "    '''Create probabilities for both derived dataframes.'''\n",
    "    \n",
    "    # Get nearest cluster from indices.\n",
    "    train_indices = kd_tree.query(oX[['longitude', 'latitude']],\n",
    "                                  return_distance=False,\n",
    "                                  dualtree=True).flatten()\n",
    "\n",
    "    # Merge indices with index\n",
    "    train_cluster_df = pd.DataFrame({'cluster': train_indices}, index=oX.index)\n",
    "\n",
    "    # Join df to eliminate cluster and add probs. \n",
    "    train_dbscan_probs = train_cluster_df.join(dbscan_df,\n",
    "                                               how='left',\n",
    "                                               on='cluster')[['dbscan_avg_high',\n",
    "                                                              'dbscan_avg_low',\n",
    "                                                              'dbscan_avg_medium']]\n",
    "\n",
    "    # Append probabilities to original derived database for train\n",
    "    odX = pd.concat([odX, train_dbscan_probs], axis=1)\n",
    "    \n",
    "    # Get the index for the closest cluster center for test.\n",
    "    test_indices = kd_tree.query(tX[['longitude', 'latitude']],\n",
    "                                 return_distance=False,\n",
    "                                 dualtree=True).flatten()\n",
    "    \n",
    "    # Merge indices with index\n",
    "    test_cluster_df = pd.DataFrame({'cluster': test_indices}, index=tX.index)\n",
    "\n",
    "    # Join df to eliminate cluster and add probs. \n",
    "    test_dbscan_probs = test_cluster_df.join(dbscan_df,\n",
    "                                             how='left',\n",
    "                                             on='cluster')[['dbscan_avg_high',\n",
    "                                                            'dbscan_avg_low',\n",
    "                                                            'dbscan_avg_medium']]\n",
    "\n",
    "    # Append probabilities to original derived database\n",
    "    tdX = pd.concat([tdX, test_dbscan_probs], axis=1)\n",
    "\n",
    "    return [oy, oX, odX, tX, tdX]\n",
    "\n",
    "\n",
    "all_X = create_dbscan_probabilities(*all_X, dbscan_df, kd_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_feat_high</th>\n",
       "      <th>text_feat_low</th>\n",
       "      <th>text_feat_medium</th>\n",
       "      <th>text_desc_high</th>\n",
       "      <th>text_desc_low</th>\n",
       "      <th>text_desc_medium</th>\n",
       "      <th>dbscan_avg_high</th>\n",
       "      <th>dbscan_avg_low</th>\n",
       "      <th>dbscan_avg_medium</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7142618</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.356675</td>\n",
       "      <td>-1.203973</td>\n",
       "      <td>-2.564949</td>\n",
       "      <td>-0.282567</td>\n",
       "      <td>-1.776492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210040</th>\n",
       "      <td>-3.954089</td>\n",
       "      <td>-0.096874</td>\n",
       "      <td>-2.615197</td>\n",
       "      <td>-1.203973</td>\n",
       "      <td>-0.510826</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-2.602690</td>\n",
       "      <td>-0.730888</td>\n",
       "      <td>-0.810930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7103890</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.310155</td>\n",
       "      <td>-1.321756</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-0.105361</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.143101</td>\n",
       "      <td>-2.014903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7143442</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.182322</td>\n",
       "      <td>-1.791759</td>\n",
       "      <td>-1.696449</td>\n",
       "      <td>-0.836248</td>\n",
       "      <td>-0.958850</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.271934</td>\n",
       "      <td>-1.435085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860601</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-0.318454</td>\n",
       "      <td>-1.299283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text_feat_high  text_feat_low  text_feat_medium  text_desc_high  \\\n",
       "listing_id                                                                    \n",
       "7142618               -inf       0.000000              -inf            -inf   \n",
       "7210040          -3.954089      -0.096874         -2.615197       -1.203973   \n",
       "7103890               -inf      -0.310155         -1.321756            -inf   \n",
       "7143442               -inf      -0.182322         -1.791759       -1.696449   \n",
       "6860601               -inf       0.000000              -inf            -inf   \n",
       "\n",
       "            text_desc_low  text_desc_medium  dbscan_avg_high  dbscan_avg_low  \\\n",
       "listing_id                                                                     \n",
       "7142618         -0.356675         -1.203973        -2.564949       -0.282567   \n",
       "7210040         -0.510826         -2.302585        -2.602690       -0.730888   \n",
       "7103890         -2.302585         -0.105361             -inf       -0.143101   \n",
       "7143442         -0.836248         -0.958850             -inf       -0.271934   \n",
       "6860601          0.000000              -inf             -inf       -0.318454   \n",
       "\n",
       "            dbscan_avg_medium  \n",
       "listing_id                     \n",
       "7142618             -1.776492  \n",
       "7210040             -0.810930  \n",
       "7103890             -2.014903  \n",
       "7143442             -1.435085  \n",
       "6860601             -1.299283  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_X[4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_room_pricing_features(oy, oX, odX, tX, tdX):\n",
    "    '''Add value-ish features to dataframe.'''\n",
    "    # Create features\n",
    "    train_pricing_df = pd.DataFrame({'dollars_per_bedroom': oX['bedrooms']/oX['price'],\n",
    "                                     'dollars_per_bathroom': oX['bathrooms']/ oX['price'],\n",
    "                                     'bathroom_percentage': oX['bathrooms'] / (oX['bedrooms'] + oX['bathrooms']),\n",
    "                                     'price': oX['price'],\n",
    "                                     'bathrooms': oX['bathrooms'],\n",
    "                                     'bedrooms': oX['bedrooms']},\n",
    "                                     index=oX.index)\n",
    "    # Percentiles \n",
    "    train_pricing_df = (train_pricing_df.rank(axis=1, pct=True, method='dense')\n",
    "                                        .applymap(lambda x: np.log(x)))\n",
    "\n",
    "    # Append to original original derived dataframe\n",
    "    odX = pd.concat([odX, train_pricing_df], axis=1)\n",
    "    \n",
    "    # Create features\n",
    "    test_pricing_df = pd.DataFrame({'dollars_per_bedroom': tX['bedrooms']/ tX['price'],\n",
    "                                    'dollars_per_bathroom': tX['bathrooms']/ tX['price'],\n",
    "                                    'bathroom_percentage': tX['bathrooms'] / (tX['bedrooms'] + tX['bathrooms']),\n",
    "                                    'price': tX['price'],\n",
    "                                    'bathrooms': tX['bathrooms'],\n",
    "                                    'bedrooms': tX['bedrooms']},\n",
    "                                    index=tX.index)\n",
    "    # Percentiles\n",
    "    test_pricing_df = (test_pricing_df.rank(axis=1, pct=True, method='dense')\n",
    "                                      .applymap(lambda x: np.log(x)))\n",
    "\n",
    "    # Append to original original derived dataframe\n",
    "    tdX = pd.concat([tdX, test_pricing_df], axis=1)\n",
    "    \n",
    "    return [oy, oX, odX, tX, tdX]\n",
    "\n",
    "\n",
    "all_X = add_room_pricing_features(*all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_X[4].head()\n",
    "cp_odX = all_X[2].copy()\n",
    "cp_tdX = all_X[4].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72633882  0.72500152  0.72541033]\n"
     ]
    }
   ],
   "source": [
    "def create_boosted_clf(oy, oX, odX, tX, tdX):\n",
    "    '''Create gbc, train gbc, cross validate, and return gbc.'''\n",
    "    # Cast negative infinity to something model can work with\n",
    "    odX = odX.replace(-np.inf, np.float32(9.9e-25))\n",
    "    tdX = tdX.replace(-np.inf, np.float32(9.9e-25))\n",
    "    # Fill na values with whatever.\n",
    "    odX = odX.fillna(odX.mean())\n",
    "    tdX = tdX.fillna(tdX.mean())\n",
    "    le = LabelEncoder().fit(oy)\n",
    "    # Grid search\n",
    "    params = {#'loss': ['deviance', 'exponential'],\n",
    "              # 'learning_rate': [1, 0.1, .01],\n",
    "              # 'n_estimators': [100, 250],\n",
    "              'subsample': [1.0, .5], \n",
    "              #'min_samples_split': [2, 5],\n",
    "              #'min_samples_leaf': [1, 2, 5],\n",
    "              'min_weight_fraction_leaf': [0.0, .2],\n",
    "              #'max_depth': [1, 3, 5],\n",
    "              'min_impurity_split': [1e-05, 1e-09]}\n",
    "    gbc = GridSearchCV(GradientBoostingClassifier(),\n",
    "                       params,\n",
    "                       scoring='neg_log_loss',\n",
    "                       n_jobs=3)\n",
    "    gbc.fit(odX, le.transform(oy))\n",
    "    # Score\n",
    "    print(cross_val_score(gbc,\n",
    "                          odX,\n",
    "                          le.transform(oy),\n",
    "                          scoring='accuracy'))\n",
    "    \n",
    "    return ([oy, oX, odX, tX, tdX], gbc, le)\n",
    "\n",
    "    \n",
    "all_X, gbc, le = create_boosted_clf(*all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=3,\n",
       "       param_grid={'subsample': [1.0, 0.5], 'min_weight_fraction_leaf': [0.0, 0.2], 'min_impurity_split': [1e-05, 1e-09]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>medium</th>\n",
       "      <th>low</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7142618</th>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.178271</td>\n",
       "      <td>0.811537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210040</th>\n",
       "      <td>0.127037</td>\n",
       "      <td>0.103861</td>\n",
       "      <td>0.769102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7103890</th>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.487031</td>\n",
       "      <td>0.510918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7143442</th>\n",
       "      <td>0.008680</td>\n",
       "      <td>0.445127</td>\n",
       "      <td>0.546193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860601</th>\n",
       "      <td>0.035208</td>\n",
       "      <td>0.137436</td>\n",
       "      <td>0.827356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                high    medium       low\n",
       "listing_id                              \n",
       "7142618     0.010191  0.178271  0.811537\n",
       "7210040     0.127037  0.103861  0.769102\n",
       "7103890     0.002051  0.487031  0.510918\n",
       "7143442     0.008680  0.445127  0.546193\n",
       "6860601     0.035208  0.137436  0.827356"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output_predictions(oy, oX, odX, tX, tdX, gbc, le):\n",
    "    '''Create the actual predictions and write to csv.'''\n",
    "    # Run predictions and write to csv\n",
    "    predictions = pd.DataFrame(gbc.predict_proba(tdX.values),\n",
    "                               columns=le.classes_,\n",
    "                               index=tdX.index)\n",
    "    predictions = predictions[['high', 'medium', 'low']]\n",
    "    predictions.to_csv('renthop_predictions.csv',\n",
    "                       index_label='listing_id')\n",
    "    return predictions\n",
    "\n",
    "\n",
    "predictions = output_predictions(*all_X, gbc, le)\n",
    "predictions.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
